# -*- coding: utf-8 -*-
"""Metrics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/142y0Uvc5stHCEY7jhz4ON6BIaFYeNU0w
"""

from google.colab import files
uploaded = files.upload()

import numpy as np
data = np.load('cuprite512 (2).npy')

print(data.shape)

import torch
data_cube = np.transpose(data, (2, 0, 1))
# adapt 3d numpy data to 5d pytorch format
data_tensor = torch.from_numpy(data_cube).unsqueeze(0).unsqueeze(0).float()

import math
from typing import Iterable, Optional, Tuple

import numpy as np
import torch


def entropy_volume(
    volume: np.ndarray,
    bins: int = 256,
    data_range: Optional[Tuple[float, float]] = None,
) -> float:
    """
    Estimate the Shannon entropy of a single hyperspectral volume.

    Parameters
    ----------
    volume : np.ndarray
        Array of intensities with arbitrary dimensionality.
    bins : int, optional
        Number of histogram bins for the empirical distribution, by default 256.
    data_range : tuple[float, float], optional
        Explicit intensity bounds to use for the histogram. When omitted, the
        bounds are derived from ``volume.min()`` and ``volume.max()``. If the
        volume is constant the entropy is zero.

    Returns
    -------
    float
        Shannon entropy in bits for the empirical distribution over the given bins.
    """
    data = volume.astype(np.float64)

    if data_range is None:
        vmin = float(np.min(data))
        vmax = float(np.max(data))
        if math.isclose(vmin, vmax):
            return 0.0
        hist_range = (vmin, vmax)
    else:
        hist_range = data_range

    hist, _ = np.histogram(data.ravel(), bins=bins, range=hist_range, density=False)
    total = hist.sum()
    if total == 0:
        return 0.0

    probs = hist.astype(np.float64) / float(total)
    probs = probs[probs > 0.0]
    if probs.size == 0:
        return 0.0
    return float(-np.sum(probs * np.log2(probs)))


def _iterate_batch(
    tensors: torch.Tensor,
    channel_index: int = 0,
) -> Iterable[np.ndarray]:
    """
    Utility generator yielding per-sample numpy views from a BCHWD tensor.
    """
    for sample in tensors:
        volume = sample[channel_index].detach().cpu().numpy()
        yield volume


def compute_batch_entropy(
    tensors: torch.Tensor,
    bins: int = 256,
    data_range: Optional[Tuple[float, float]] = None,
) -> float:
    """
    Compute the mean Shannon entropy across a batch of hyperspectral cubes.

    Parameters
    ----------
    tensors : torch.Tensor
        Input tensor with shape (B, C, D, H, W). The entropy is evaluated
        on the first channel (C dimension) to mirror the existing HSI metrics.
    bins : int, optional
        Number of histogram bins to approximate the distribution, by default 256.
    data_range : tuple[float, float], optional
        Explicit histogram bounds. When ``None`` the range is inferred per-sample.

    Returns
    -------
    float
        Average entropy over the batch in bits.
    """
    assert tensors.ndim == 5, f"Expected a 5D tensor (B, C, D, H, W), but got {tensors.shape}"

    entropies = [
        entropy_volume(volume, bins=bins, data_range=data_range)
        for volume in _iterate_batch(tensors)
    ]
    if not entropies:
        return 0.0
    return float(np.mean(entropies))


def compute_residual_entropy(
    predictions: torch.Tensor,
    targets: torch.Tensor,
    bins: int = 256,
    data_range: Optional[Tuple[float, float]] = None,
) -> float:
    """
    Compute the mean entropy of residual volumes (prediction - target) over a batch.

    Parameters
    ----------
    predictions : torch.Tensor
        Predicted hyperspectral cubes, shape (B, C, D, H, W).
    targets : torch.Tensor
        Ground-truth hyperspectral cubes, same shape as ``predictions``.
    bins : int, optional
        Number of histogram bins for the entropy estimate, by default 256.
    data_range : tuple[float, float], optional
        Explicit histogram bounds. ``None`` falls back to per-sample min/max.

    Returns
    -------
    float
        Average entropy of the residual volumes in bits.
    """
    assert predictions.shape == targets.shape, "Predictions and targets must have the same shape"
    assert predictions.ndim == 5, f"Expected 5D tensors (B, C, D, H, W), but got {predictions.shape}"

    residuals = predictions - targets
    entropies = [
        entropy_volume(volume, bins=bins, data_range=data_range)
        for volume in _iterate_batch(residuals)
    ]
    if not entropies:
        return 0.0
    return float(np.mean(entropies))

# via changing format of numpy datacube to 5d pytorch format
entropy = compute_batch_entropy(data_tensor)
print(f"Batch entropy: {entropy:.4f} bits")

# without changing format of numpy datacube
entropy = entropy_volume(data, bins=256)
print(f"Entropy of full hyperspectral cube: {entropy:.4f} bits")

# Initialize list to store SNR per band
snr_values = []

for i in range(data_cube.shape[0]):
    band = data_cube[i, :, :]  # 2D slice

    # Compute signal mean and noise std
    signal_mean = np.mean(band)
    noise_std = np.std(band)

    # Avoid divide-by-zero
    snr = signal_mean / noise_std if noise_std != 0 else 0
    snr_values.append(snr)

# Convert to numpy array
snr_values = np.array(snr_values)

for i in range(len(snr_values)): #iterating over array's indexes
  band_number = i + 1 #because index starts at zero but we want it to start at band 1
  print (f"band: {band_number} ", snr_values[i])

# calculate the mean of all SNR values
mean_snr = np.mean(snr_values)
print(f"Average SNR across all bands: {mean_snr:.4f}")

import matplotlib.pyplot as plt

# visualize the results
plt.figure(figsize=(8, 4))
plt.plot(range(1, len(snr_values) + 1), snr_values, linewidth=1)
plt.xlabel("Spectral Band Index")
plt.ylabel("SNR")
plt.title("SNR per Spectral Band for cuprite datacube")
plt.grid(True)
plt.show()

# Entropyâ€“SNR comparison table
# higher entropy -> noisier signal -> lower SNR
print(f"Entropy (bits): {entropy:.4f}")
print(f"Average SNR: {mean_snr:.4f}")